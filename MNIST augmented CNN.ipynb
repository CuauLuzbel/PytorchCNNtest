{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nex = iter(train_loader).next()\\nex[0].shape\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = datasets.MNIST(root = \"./data/\", \n",
    "                               train = True, \n",
    "                               transform = transform,\n",
    "                               download = True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root = \"./data/\",\n",
    "                              train = False,\n",
    "                              transform = transform)\n",
    "\n",
    "\n",
    "# Train indices for validation\n",
    "validation_size = 0.2\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(validation_size * num_train))\n",
    "train_idx, validation_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    " \n",
    "# Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           sampler = train_sampler)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                batch_size = batch_size,\n",
    "                                                sampler = validation_sampler)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = False)\n",
    "\n",
    "''' \n",
    "ex = iter(train_loader).next()\n",
    "ex[0].shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create swish function\n",
    "from torch.autograd import Function\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def swish(input):\n",
    "    return input * torch.sigmoid(input)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return swish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuausuarez/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:118: UserWarning: \n",
      "    Found GPU0 GeForce GTX 870M which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 3)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 3)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size = 3)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(30, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p = 0.2)\n",
    "        self.dropout2 = nn.Dropout(p = 0.4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        \n",
    "        out = swish(self.maxpool(self.conv1(x)))\n",
    "        out = swish(self.maxpool(self.conv2(out)))\n",
    "        out = swish(self.maxpool(self.conv3(out)))\n",
    "        \n",
    "        out = out.view(in_size, -1)\n",
    "        \n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = swish(self.fc1(out))\n",
    "        \n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = Net()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss 2.271020 \tValidation Loss: 1.635013\n",
      "Validation loss decreased from inf to 1.635013. Saving model!\n",
      "Epoch: 2 \tTraining Loss 1.052034 \tValidation Loss: 0.578856\n",
      "Validation loss decreased from 1.635013 to 0.578856. Saving model!\n",
      "Epoch: 3 \tTraining Loss 0.687932 \tValidation Loss: 0.407287\n",
      "Validation loss decreased from 0.578856 to 0.407287. Saving model!\n",
      "Epoch: 4 \tTraining Loss 0.568072 \tValidation Loss: 0.322811\n",
      "Validation loss decreased from 0.407287 to 0.322811. Saving model!\n",
      "Epoch: 5 \tTraining Loss 0.494737 \tValidation Loss: 0.264706\n",
      "Validation loss decreased from 0.322811 to 0.264706. Saving model!\n",
      "Epoch: 6 \tTraining Loss 0.433639 \tValidation Loss: 0.238519\n",
      "Validation loss decreased from 0.264706 to 0.238519. Saving model!\n",
      "Epoch: 7 \tTraining Loss 0.404708 \tValidation Loss: 0.231005\n",
      "Validation loss decreased from 0.238519 to 0.231005. Saving model!\n",
      "Epoch: 8 \tTraining Loss 0.371949 \tValidation Loss: 0.217952\n",
      "Validation loss decreased from 0.231005 to 0.217952. Saving model!\n",
      "Epoch: 9 \tTraining Loss 0.356178 \tValidation Loss: 0.200460\n",
      "Validation loss decreased from 0.217952 to 0.200460. Saving model!\n",
      "Epoch: 10 \tTraining Loss 0.338185 \tValidation Loss: 0.203453\n",
      "Epoch: 11 \tTraining Loss 0.326877 \tValidation Loss: 0.180135\n",
      "Validation loss decreased from 0.200460 to 0.180135. Saving model!\n",
      "Epoch: 12 \tTraining Loss 0.311403 \tValidation Loss: 0.176442\n",
      "Validation loss decreased from 0.180135 to 0.176442. Saving model!\n",
      "Epoch: 13 \tTraining Loss 0.303179 \tValidation Loss: 0.170453\n",
      "Validation loss decreased from 0.176442 to 0.170453. Saving model!\n",
      "Epoch: 14 \tTraining Loss 0.295471 \tValidation Loss: 0.167628\n",
      "Validation loss decreased from 0.170453 to 0.167628. Saving model!\n",
      "Epoch: 15 \tTraining Loss 0.284859 \tValidation Loss: 0.172137\n",
      "Epoch: 16 \tTraining Loss 0.283497 \tValidation Loss: 0.163995\n",
      "Validation loss decreased from 0.167628 to 0.163995. Saving model!\n",
      "Epoch: 17 \tTraining Loss 0.273660 \tValidation Loss: 0.164837\n",
      "Epoch: 18 \tTraining Loss 0.265524 \tValidation Loss: 0.149996\n",
      "Validation loss decreased from 0.163995 to 0.149996. Saving model!\n",
      "Epoch: 19 \tTraining Loss 0.263855 \tValidation Loss: 0.151842\n",
      "Epoch: 20 \tTraining Loss 0.264077 \tValidation Loss: 0.141468\n",
      "Validation loss decreased from 0.149996 to 0.141468. Saving model!\n",
      "Epoch: 21 \tTraining Loss 0.255530 \tValidation Loss: 0.143550\n",
      "Epoch: 22 \tTraining Loss 0.247744 \tValidation Loss: 0.149648\n",
      "Epoch: 23 \tTraining Loss 0.243697 \tValidation Loss: 0.137517\n",
      "Validation loss decreased from 0.141468 to 0.137517. Saving model!\n",
      "Epoch: 24 \tTraining Loss 0.243653 \tValidation Loss: 0.147876\n",
      "Epoch: 25 \tTraining Loss 0.240908 \tValidation Loss: 0.141044\n",
      "Epoch: 26 \tTraining Loss 0.236605 \tValidation Loss: 0.146976\n",
      "Epoch: 27 \tTraining Loss 0.233719 \tValidation Loss: 0.140412\n",
      "Epoch: 28 \tTraining Loss 0.235447 \tValidation Loss: 0.143207\n",
      "Epoch: 29 \tTraining Loss 0.237014 \tValidation Loss: 0.133683\n",
      "Validation loss decreased from 0.137517 to 0.133683. Saving model!\n",
      "Epoch: 30 \tTraining Loss 0.230631 \tValidation Loss: 0.149780\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "validation_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Track loss\n",
    "    train_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    \n",
    "    #########\n",
    "    # Train #\n",
    "    #########\n",
    "    model.train()\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "    ############\n",
    "    # Validate #\n",
    "    ############\n",
    "    model.eval()\n",
    "\n",
    "    for data, target in validation_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        validation_loss += loss.item() * data.size(0)\n",
    "\n",
    "    # Average losses\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    validation_loss = validation_loss / len(validation_loader.sampler)\n",
    "\n",
    "    print(\"Epoch: {} \\tTraining Loss {:.6f} \\tValidation Loss: {:.6f}\".format( \n",
    "          epoch, train_loss, validation_loss))\n",
    "\n",
    "    # Save model\n",
    "    if validation_loss <= validation_loss_min:\n",
    "        print(\"Validation loss decreased from {:.6f} to {:.6f}. Saving model!\".format(\n",
    "             validation_loss_min,\n",
    "             validation_loss))\n",
    "\n",
    "        torch.save(model.state_dict(), 'model_mnist.pt')\n",
    "        validation_loss_min = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model with lowest validation loss\n",
    "model.load_state_dict(torch.load('model_mnist.pt'), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight', tensor([[[[ 0.2078, -0.4234, -0.9413],\n",
       "                        [-0.6778, -0.0064,  0.3089],\n",
       "                        [-0.8366, -0.0190,  0.8278]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4194, -0.3224, -0.8077],\n",
       "                        [ 0.8938,  0.3903, -1.1121],\n",
       "                        [ 0.8251, -0.4956, -1.0328]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1737,  0.0860,  0.2198],\n",
       "                        [ 0.0352, -0.0606,  0.1698],\n",
       "                        [ 0.2603,  0.2646,  0.3119]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.6985, -0.9129, -0.5467],\n",
       "                        [ 0.2243, -0.0689, -1.1157],\n",
       "                        [ 0.7689,  0.8681,  0.3506]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0919, -0.7920, -0.1852],\n",
       "                        [-0.8336, -0.4799,  0.7122],\n",
       "                        [ 0.1597,  0.7769,  0.7203]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0900, -0.0138, -0.0582],\n",
       "                        [-0.6569, -0.7837, -0.1592],\n",
       "                        [ 0.1284, -0.6341, -1.3078]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4409,  0.0503, -0.2602],\n",
       "                        [ 0.1213,  0.7247,  0.1060],\n",
       "                        [-1.0932, -0.7692,  0.3778]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.5289, -0.2137, -0.9308],\n",
       "                        [ 0.0179,  1.0995, -0.4651],\n",
       "                        [-0.4369,  0.2562,  0.7474]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1445, -0.1052, -0.4353],\n",
       "                        [-0.0702, -0.2171, -0.3372],\n",
       "                        [-0.1444, -0.3563, -0.0695]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2945,  0.1134,  0.3979],\n",
       "                        [-0.5364, -0.1392,  0.8869],\n",
       "                        [-0.8221, -0.5341,  0.7382]]]], device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.0715, -0.3935, -0.2266, -0.3718, -0.4387,  0.4645, -0.2291, -0.7667,\n",
       "                       0.3436, -0.4783], device='cuda:0')),\n",
       "             ('conv2.weight', tensor([[[[ 0.0802, -0.1991,  0.1150],\n",
       "                        [-0.1487,  0.0500, -0.0145],\n",
       "                        [ 0.0491,  0.0225, -0.1276]],\n",
       "              \n",
       "                       [[-0.0620,  0.1623,  0.0705],\n",
       "                        [-0.3292,  0.0048,  0.0329],\n",
       "                        [-0.0185,  0.0229, -0.0857]],\n",
       "              \n",
       "                       [[-0.3509,  0.0235,  0.2438],\n",
       "                        [ 0.0126, -0.0368,  0.1285],\n",
       "                        [ 0.0677,  0.0604,  0.6004]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.2614,  0.3110,  0.1422],\n",
       "                        [-0.2029, -0.0324,  0.2192],\n",
       "                        [-0.2315, -0.1542,  0.3850]],\n",
       "              \n",
       "                       [[-0.0100, -0.0223, -0.0449],\n",
       "                        [-0.3130, -0.2260, -0.1698],\n",
       "                        [-0.2160, -0.4041, -0.6069]],\n",
       "              \n",
       "                       [[ 0.0913,  0.1961, -0.1848],\n",
       "                        [-0.0238,  0.0644,  0.1899],\n",
       "                        [-0.1202,  0.0148,  0.3700]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0879,  0.0486, -0.1460],\n",
       "                        [ 0.1997,  0.1931, -0.0286],\n",
       "                        [ 0.1483,  0.1243,  0.2391]],\n",
       "              \n",
       "                       [[ 0.1137, -0.1267, -0.3569],\n",
       "                        [ 0.0442, -0.1237, -0.4509],\n",
       "                        [ 0.1489,  0.1328, -0.0490]],\n",
       "              \n",
       "                       [[-0.4259, -0.3247, -0.2580],\n",
       "                        [-0.1982,  0.0933,  0.2806],\n",
       "                        [ 0.3231,  0.1294, -0.8906]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.2384, -0.2491,  0.1383],\n",
       "                        [-0.1906, -0.1596, -0.0886],\n",
       "                        [-0.1831, -0.2429, -0.3214]],\n",
       "              \n",
       "                       [[ 0.0167,  0.0976, -0.1309],\n",
       "                        [ 0.0301,  0.0331, -0.0652],\n",
       "                        [ 0.0356, -0.0485,  0.0720]],\n",
       "              \n",
       "                       [[-0.1003, -0.1705, -0.1927],\n",
       "                        [-0.0684, -0.2201, -0.2651],\n",
       "                        [ 0.0485, -0.0788, -0.2907]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0269,  0.2406,  0.0162],\n",
       "                        [-0.0048, -0.1114, -0.0090],\n",
       "                        [-0.0898, -0.0963, -0.0111]],\n",
       "              \n",
       "                       [[ 0.2286,  0.1004,  0.1394],\n",
       "                        [ 0.1387,  0.1299,  0.1817],\n",
       "                        [ 0.2813,  0.2603,  0.3279]],\n",
       "              \n",
       "                       [[-0.4332,  0.3508, -0.0255],\n",
       "                        [-0.4329,  0.2262, -0.2976],\n",
       "                        [-0.2243,  0.2566, -0.2637]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.3599, -0.3798, -0.1138],\n",
       "                        [-0.2044,  0.0124, -0.0252],\n",
       "                        [-0.0712,  0.0712,  0.4148]],\n",
       "              \n",
       "                       [[ 0.1527, -0.0058, -0.1098],\n",
       "                        [ 0.1125,  0.0748, -0.1081],\n",
       "                        [-0.0499,  0.0388, -0.1425]],\n",
       "              \n",
       "                       [[ 0.0407, -0.0257,  0.0680],\n",
       "                        [-0.0073,  0.1977,  0.1444],\n",
       "                        [ 0.1211,  0.4237,  0.1511]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.1097, -0.1291, -0.0532],\n",
       "                        [-0.1826, -0.0975, -0.0187],\n",
       "                        [-0.0580,  0.0489,  0.1381]],\n",
       "              \n",
       "                       [[-0.1210, -0.1689,  0.0366],\n",
       "                        [-0.2280, -0.3868, -0.1832],\n",
       "                        [ 0.1636,  0.0689,  0.2145]],\n",
       "              \n",
       "                       [[ 0.1734,  0.2338,  0.2807],\n",
       "                        [ 0.0222,  0.1057,  0.0640],\n",
       "                        [-0.3898, -0.5830, -0.5771]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.2182,  0.2651, -0.1844],\n",
       "                        [ 0.1232,  0.1100, -0.2435],\n",
       "                        [ 0.1619, -0.1398, -0.3581]],\n",
       "              \n",
       "                       [[ 0.0451,  0.0926,  0.0540],\n",
       "                        [-0.1256, -0.2110, -0.0293],\n",
       "                        [ 0.1172,  0.0436,  0.2200]],\n",
       "              \n",
       "                       [[ 0.1856,  0.0225, -0.1794],\n",
       "                        [-0.0498, -0.1388, -0.2609],\n",
       "                        [ 0.2132,  0.0374,  0.1504]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0439,  0.0434,  0.0190],\n",
       "                        [ 0.0966,  0.0834, -0.0828],\n",
       "                        [ 0.0018, -0.0779, -0.0323]],\n",
       "              \n",
       "                       [[-0.2983, -0.2431, -0.0329],\n",
       "                        [-0.2394, -0.0979,  0.0357],\n",
       "                        [-0.3712, -0.0060, -0.1073]],\n",
       "              \n",
       "                       [[-0.7058, -0.4107, -0.7094],\n",
       "                        [-0.4137, -0.2348, -0.4756],\n",
       "                        [-0.4701, -0.3909, -0.1138]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.2920, -0.2583, -0.2093],\n",
       "                        [-0.2204, -0.0165, -0.1829],\n",
       "                        [-0.2533, -0.2159,  0.0367]],\n",
       "              \n",
       "                       [[ 0.1697,  0.1302,  0.1080],\n",
       "                        [ 0.0648,  0.0157, -0.0616],\n",
       "                        [ 0.0186,  0.0188, -0.0469]],\n",
       "              \n",
       "                       [[-0.1200, -0.2638, -0.3843],\n",
       "                        [-0.0780, -0.2005,  0.0272],\n",
       "                        [-0.2300, -0.1481, -0.1219]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0743,  0.0617,  0.2350],\n",
       "                        [-0.1125,  0.2740,  0.2085],\n",
       "                        [-0.0747,  0.0819, -0.1040]],\n",
       "              \n",
       "                       [[-0.1110, -0.0614,  0.0240],\n",
       "                        [-0.0231, -0.0148, -0.0069],\n",
       "                        [ 0.1199,  0.0535, -0.1814]],\n",
       "              \n",
       "                       [[ 0.0319,  0.0430,  0.3250],\n",
       "                        [ 0.3189,  0.3467, -0.1512],\n",
       "                        [ 0.3057, -0.0145, -0.5220]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0608,  0.0655, -0.2639],\n",
       "                        [-0.0317, -0.3234, -0.2899],\n",
       "                        [-0.0496,  0.0293,  0.1523]],\n",
       "              \n",
       "                       [[-0.1499, -0.1968,  0.1144],\n",
       "                        [-0.0960, -0.0460, -0.0057],\n",
       "                        [-0.1065, -0.1273,  0.0389]],\n",
       "              \n",
       "                       [[ 0.0088,  0.2514,  0.0488],\n",
       "                        [-0.0591, -0.0703, -0.5194],\n",
       "                        [-0.1908,  0.0802, -0.1071]]]], device='cuda:0')),\n",
       "             ('conv2.bias',\n",
       "              tensor([-0.8444, -0.3515, -0.2898, -0.5281, -0.4420, -0.6344, -0.6879, -0.5955,\n",
       "                      -0.2525, -0.3761, -0.4487, -0.3227, -0.9651, -0.3060, -0.5697, -0.4610,\n",
       "                      -0.4083,  0.0582, -0.6035, -0.1905], device='cuda:0')),\n",
       "             ('conv3.weight',\n",
       "              tensor([[[[-5.6853e-02, -9.3008e-02, -1.5671e-01],\n",
       "                        [ 1.4344e-01, -4.3029e-01, -3.7586e-01],\n",
       "                        [ 4.2973e-01,  7.5267e-02,  7.1767e-03]],\n",
       "              \n",
       "                       [[-1.5240e-01, -4.3465e-02, -8.4274e-02],\n",
       "                        [ 3.5313e-01,  2.4808e-01, -2.3393e-01],\n",
       "                        [ 3.4914e-02, -1.9659e-01, -1.1317e-01]],\n",
       "              \n",
       "                       [[ 8.8478e-02,  1.3829e-01,  1.7876e-01],\n",
       "                        [-1.2226e-01, -1.4454e-01,  8.6542e-02],\n",
       "                        [-3.9405e-01, -5.9923e-02, -1.4412e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.3502e-02,  5.5637e-02, -7.3303e-02],\n",
       "                        [ 4.2617e-02,  1.2943e-01, -7.4810e-02],\n",
       "                        [-2.3622e-01, -2.6690e-01, -3.7625e-01]],\n",
       "              \n",
       "                       [[-1.0599e-01,  2.0851e-01, -1.5579e-02],\n",
       "                        [-7.6816e-02, -3.2898e-02,  2.7228e-02],\n",
       "                        [-1.8043e-01, -1.9181e-01, -2.4536e-01]],\n",
       "              \n",
       "                       [[ 3.1579e-01,  5.9386e-02, -1.3346e-01],\n",
       "                        [-2.8851e-01, -2.7998e-01, -2.5435e-01],\n",
       "                        [ 4.5839e-02,  5.0218e-02, -1.9530e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1647e-02, -2.4276e-01, -1.2632e-01],\n",
       "                        [ 4.1947e-02,  5.6687e-03, -2.4439e-01],\n",
       "                        [-1.0673e-01,  2.7944e-01, -3.2130e-01]],\n",
       "              \n",
       "                       [[-2.7217e-01,  5.4452e-02,  6.3441e-02],\n",
       "                        [ 9.2463e-02, -3.2517e-01,  2.0117e-01],\n",
       "                        [ 1.6345e-01, -1.1037e-01,  4.7536e-02]],\n",
       "              \n",
       "                       [[ 7.0132e-02,  2.0075e-01,  8.2036e-02],\n",
       "                        [-1.1700e-01, -3.3821e-01, -9.6873e-02],\n",
       "                        [ 2.0618e-01,  7.5917e-02, -8.3022e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.7759e-02, -1.1554e-01, -1.6575e-01],\n",
       "                        [ 2.6110e-01, -2.5078e-01,  2.6715e-01],\n",
       "                        [ 1.3713e-02, -1.7012e-01, -2.3483e-01]],\n",
       "              \n",
       "                       [[-2.3588e-01, -2.6972e-01, -2.1400e-01],\n",
       "                        [ 4.7237e-03,  2.6336e-01,  3.0983e-02],\n",
       "                        [ 8.7022e-02, -3.2350e-01,  1.3023e-01]],\n",
       "              \n",
       "                       [[ 4.7641e-02,  1.0235e-01,  7.2670e-02],\n",
       "                        [-4.6026e-02, -7.6773e-02,  4.5335e-01],\n",
       "                        [ 3.4628e-01, -2.8403e-02, -3.0181e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.7516e-02, -1.2985e-01, -6.5544e-02],\n",
       "                        [-2.4306e-03,  2.3139e-01, -1.2157e-01],\n",
       "                        [-2.0549e-01,  1.2997e-02, -2.9177e-01]],\n",
       "              \n",
       "                       [[-1.3889e-01,  1.2617e-01, -1.6416e-01],\n",
       "                        [-2.3476e-01, -1.9179e-01,  1.7443e-01],\n",
       "                        [ 4.0997e-01, -1.1802e-01, -1.2212e-01]],\n",
       "              \n",
       "                       [[ 3.1604e-02, -3.0754e-02, -3.6483e-01],\n",
       "                        [-1.7480e-01, -1.5440e-01, -1.9248e-01],\n",
       "                        [ 2.2054e-01,  1.2453e-01,  1.4396e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.0054e-02, -1.2889e-01, -1.9275e-01],\n",
       "                        [ 7.0263e-02, -1.5850e-01,  1.2023e-01],\n",
       "                        [-1.2184e-01, -2.6571e-03,  1.3786e-01]],\n",
       "              \n",
       "                       [[-1.9563e-01, -2.2369e-01, -3.9497e-01],\n",
       "                        [-5.9362e-02, -1.7988e-01, -6.1632e-02],\n",
       "                        [ 1.3806e-01,  1.1287e-02, -8.9090e-02]],\n",
       "              \n",
       "                       [[ 3.5763e-02,  3.2898e-02,  8.4250e-02],\n",
       "                        [ 1.0288e-01,  6.9502e-02,  5.1234e-01],\n",
       "                        [ 5.2953e-02, -2.0841e-01, -5.0310e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.3680e-01, -4.0787e-01, -6.9305e-01],\n",
       "                        [-1.6181e-01, -4.1887e-02, -8.9120e-02],\n",
       "                        [ 1.2176e-01,  9.1861e-02, -2.4604e-02]],\n",
       "              \n",
       "                       [[-2.0494e-01,  1.1296e-01,  4.6183e-03],\n",
       "                        [-1.3659e-01, -1.1074e-01,  1.8995e-01],\n",
       "                        [-3.1881e-01, -8.5806e-02, -5.5628e-01]],\n",
       "              \n",
       "                       [[-3.0411e-01, -1.5832e-01, -9.7457e-02],\n",
       "                        [-6.7080e-02, -2.0545e-02,  1.1809e-01],\n",
       "                        [-4.6113e-02, -1.2687e-02, -3.8015e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9862e-01, -2.7124e-01, -3.3838e-01],\n",
       "                        [-3.0900e-01,  2.8322e-02,  2.4823e-03],\n",
       "                        [ 1.1056e-02, -1.5272e-01,  1.1687e-01]],\n",
       "              \n",
       "                       [[-1.6776e-01,  1.1797e-01,  2.0286e-01],\n",
       "                        [-1.8989e-01, -1.5823e-02, -3.9624e-01],\n",
       "                        [-5.4858e-02,  1.3042e-01,  1.3171e-01]],\n",
       "              \n",
       "                       [[-2.1537e-01, -2.8307e-01, -4.1874e-01],\n",
       "                        [-3.4100e-01, -4.0610e-02, -4.1504e-02],\n",
       "                        [-2.2052e-01, -1.1568e-01, -3.7741e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8610e-02, -3.7451e-01, -3.6702e-02],\n",
       "                        [-1.4173e-01,  1.2178e-02, -2.5188e-01],\n",
       "                        [-5.1613e-02,  2.3567e-01,  3.4816e-01]],\n",
       "              \n",
       "                       [[ 1.1861e-02,  4.6267e-02,  1.1732e-01],\n",
       "                        [ 1.4649e-02,  7.7235e-02, -1.8421e-01],\n",
       "                        [-3.5900e-02, -2.3412e-01, -7.0321e-02]],\n",
       "              \n",
       "                       [[ 2.7299e-01,  8.0384e-02, -1.1557e-01],\n",
       "                        [-3.4459e-01, -7.9022e-02, -1.5467e-02],\n",
       "                        [-2.8144e-01, -2.4855e-01, -2.9587e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9624e-01, -8.4312e-02, -2.7277e-02],\n",
       "                        [ 6.4712e-02, -2.3857e-01, -2.2758e-01],\n",
       "                        [ 2.9479e-01, -1.4404e-01, -1.8854e-01]],\n",
       "              \n",
       "                       [[-7.0263e-02, -2.1262e-01, -4.2764e-01],\n",
       "                        [-1.7995e-01,  3.0727e-02, -8.9646e-02],\n",
       "                        [-3.0201e-01,  1.4597e-02, -6.5206e-02]],\n",
       "              \n",
       "                       [[ 1.8082e-02,  5.4297e-02, -1.7108e-01],\n",
       "                        [-6.0240e-01,  2.8069e-02,  1.1768e-01],\n",
       "                        [-3.2355e-02,  1.2551e-01,  3.3387e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9914e-01, -2.5973e-01, -2.5181e-01],\n",
       "                        [-1.4546e-01,  3.6174e-02, -2.9145e-01],\n",
       "                        [ 4.4190e-01, -3.1571e-01,  1.1574e-01]],\n",
       "              \n",
       "                       [[-8.0478e-02,  6.0202e-02,  1.7897e-01],\n",
       "                        [-7.6193e-02, -2.1626e-02,  4.2611e-05],\n",
       "                        [-9.5219e-02, -1.7455e-01, -3.9721e-02]],\n",
       "              \n",
       "                       [[-7.9882e-02,  2.8870e-02, -1.0676e-01],\n",
       "                        [ 6.5019e-02,  3.5175e-02,  1.2533e-01],\n",
       "                        [-2.4699e-01, -3.6285e-01,  1.5402e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.3244e-01, -5.1342e-01, -6.3549e-01],\n",
       "                        [-3.1622e-01, -9.3777e-02, -4.2770e-01],\n",
       "                        [ 2.2200e-01, -3.4354e-02,  2.0551e-02]],\n",
       "              \n",
       "                       [[ 2.0903e-02, -1.0356e-01,  8.7846e-02],\n",
       "                        [-2.4228e-01, -2.4104e-01, -1.4144e-01],\n",
       "                        [-4.3350e-01, -1.1150e-01,  3.9044e-03]],\n",
       "              \n",
       "                       [[-5.1905e-01, -2.0236e-01, -4.3363e-01],\n",
       "                        [-1.5409e-01,  9.4635e-02, -5.2650e-02],\n",
       "                        [ 4.2948e-01, -8.9677e-02, -5.2247e-02]]]], device='cuda:0')),\n",
       "             ('conv3.bias',\n",
       "              tensor([-0.3049, -0.2552, -0.3762, -0.3551, -0.6683, -0.9228, -0.1742, -0.7091,\n",
       "                       0.1201, -0.4171, -0.6400, -0.7413, -0.3130, -0.4682, -0.5220, -0.1930,\n",
       "                      -0.3491, -0.2678, -0.5070, -0.4594, -0.3857, -0.4043, -0.1610, -0.5386,\n",
       "                       0.0061, -0.4297, -0.1244, -0.7570, -0.4578, -0.8228], device='cuda:0')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0236,  0.4204,  0.1973,  0.1941,  0.2094, -0.0651,  0.2804, -0.2802,\n",
       "                       -0.3895,  0.0632, -0.0848, -0.0930,  0.4314, -0.3846, -0.3254, -0.5680,\n",
       "                       -0.0177, -0.2205,  0.3352, -0.1519,  0.0052, -0.4800,  0.1497, -0.0093,\n",
       "                       -0.4649,  0.0704, -0.1842,  0.1520,  0.1657, -0.3957],\n",
       "                      [ 0.0433, -0.0731, -0.2066, -0.6404, -0.2102,  0.0711, -0.5423,  0.0789,\n",
       "                        0.1840,  0.1809, -0.1162, -0.2291, -0.0192,  0.1594, -0.4920,  0.3011,\n",
       "                       -0.0633, -0.3625,  0.0167,  0.3140,  0.1122,  0.0174, -0.5878, -0.4857,\n",
       "                        0.2218, -0.2598,  0.2592, -0.0330, -0.2384,  0.3573],\n",
       "                      [ 0.2142, -0.0233, -0.3826,  0.3135,  0.0803,  0.2126,  0.3806,  0.0791,\n",
       "                       -0.6658,  0.2238, -0.5130,  0.1656,  0.1057, -0.5516, -0.2694, -0.3824,\n",
       "                        0.0044, -0.0850,  0.4824,  0.4508,  0.2154, -0.4107,  0.1994, -0.0681,\n",
       "                       -0.4948,  0.0254, -0.1801,  0.1146,  0.2006,  0.2915],\n",
       "                      [-0.4042,  0.0425,  0.2804,  0.5423, -0.6459, -0.0479, -0.3422,  0.1602,\n",
       "                       -0.2879, -0.5135,  0.2734,  0.3913,  0.0793,  0.0864,  0.0996, -0.0861,\n",
       "                       -0.6018, -0.0450,  0.1317,  0.1458, -0.3244,  0.1526, -0.4234,  0.2593,\n",
       "                       -0.3138, -0.8090, -0.1072, -0.0332,  0.0126, -0.6014],\n",
       "                      [ 0.3289,  0.0118, -0.6569,  0.7482,  0.1545, -0.4134, -0.0400, -0.5450,\n",
       "                       -0.1176,  0.0201, -0.3559, -0.5160, -0.0406,  0.0052,  0.0249,  0.1790,\n",
       "                        0.5893, -0.5423, -0.1503,  0.2015, -0.1622, -0.1072, -0.0662, -0.2224,\n",
       "                       -0.0172,  0.5326,  0.5915, -0.0223, -0.0306, -0.5957],\n",
       "                      [-0.1482, -0.4417, -0.0713, -0.4653,  0.0137,  0.4662,  0.1135,  0.2670,\n",
       "                        0.1850,  0.0470,  0.0425,  0.0591, -0.4223, -0.5066, -0.0457,  0.0875,\n",
       "                       -0.4951,  0.3864,  0.1981, -0.0750,  0.2598, -0.3842,  0.1256,  0.0929,\n",
       "                        0.1500, -0.3992, -0.3729,  0.2420, -0.1997,  0.3330],\n",
       "                      [ 0.1452, -0.0906, -0.2992, -0.2127,  0.1883, -0.0716,  0.2227, -0.3427,\n",
       "                       -0.3813,  0.1237, -0.4662,  0.7867, -0.0673,  0.0945,  0.1966, -0.3897,\n",
       "                        0.1119, -0.1784,  0.3372, -0.4139, -0.0051,  0.2704,  0.1176,  0.2244,\n",
       "                       -0.3653,  0.0883, -0.0793,  0.1949,  0.3952, -0.1050],\n",
       "                      [ 0.1938,  0.4457,  0.1867, -0.4148, -0.2524,  0.2091, -0.1994,  0.3327,\n",
       "                        0.0876,  0.3778,  0.3092, -0.3473,  0.3320, -0.2151, -0.1539,  0.3712,\n",
       "                        0.0840, -0.3491, -0.2686, -0.1975, -0.2202, -0.3168, -0.1193,  0.2247,\n",
       "                       -0.1478, -0.0211,  0.3106,  0.1756,  0.0363,  0.0850],\n",
       "                      [-0.4357,  0.1154,  0.2086,  0.3053,  0.0104, -0.6242, -0.4470, -0.3649,\n",
       "                       -0.1140, -0.3172, -0.1198,  0.1008,  0.1987,  0.3756,  0.1594,  0.0571,\n",
       "                       -0.0354, -0.3900, -0.0917,  0.3421, -0.1005,  0.4052, -0.4795, -0.1057,\n",
       "                        0.0591,  0.0421,  0.2485, -0.2811, -0.2086, -0.2503],\n",
       "                      [-0.5424, -0.4772, -0.1575, -0.2090,  0.1756,  0.0106, -0.0667,  0.0788,\n",
       "                       -0.4597, -0.4991,  0.0389,  0.5316, -0.3393,  0.2810,  0.4021,  0.1591,\n",
       "                        0.0223,  0.0219,  0.1744, -0.5110,  0.1395,  0.3500, -0.1788,  0.2272,\n",
       "                       -0.2196, -0.0073,  0.0147,  0.3511, -0.1712, -0.3138],\n",
       "                      [-0.3422,  0.1307,  0.2873,  0.3118,  0.0386,  0.3200,  0.1238,  0.4198,\n",
       "                       -0.5508, -0.0816,  0.0615, -0.1440,  0.2697, -0.4568, -0.1636,  0.0165,\n",
       "                       -0.5771, -0.1146,  0.3190,  0.3198,  0.2329, -0.3722, -0.1807,  0.0321,\n",
       "                       -0.4526, -0.2851, -0.2038,  0.3059, -0.3206,  0.2014],\n",
       "                      [-0.1229, -0.2811, -0.1751,  0.4213, -0.4386, -0.1768,  0.1079, -0.3775,\n",
       "                        0.2682, -0.1770,  0.0712, -0.0759, -0.2238, -0.1221, -0.5776,  0.1320,\n",
       "                       -0.0930,  0.0969,  0.0863,  0.3173, -0.4225, -0.1467,  0.0546, -0.8865,\n",
       "                        0.3655, -0.3983,  0.3188, -0.4448, -0.2990, -0.2388],\n",
       "                      [-0.3975, -0.3631, -0.1259,  0.2658,  0.1343,  0.0890,  0.3102, -0.4125,\n",
       "                        0.1268, -0.4260, -0.3777,  0.2431, -0.1215, -0.6333, -0.3469, -0.5008,\n",
       "                       -0.6160,  0.2317,  0.3536,  0.1905,  0.1262, -0.4378,  0.2539, -0.1737,\n",
       "                        0.2046, -0.4140, -0.0721,  0.0685, -0.1019, -0.1722],\n",
       "                      [ 0.3109,  0.0263, -0.4856, -0.4993,  0.1061, -0.0400, -0.2048,  0.0201,\n",
       "                       -0.0024,  0.2387, -0.2530, -0.5267, -0.0821,  0.0688,  0.0287,  0.2433,\n",
       "                        0.5025, -0.4705, -0.5003,  0.1007,  0.1185,  0.0083, -0.0450, -0.3048,\n",
       "                        0.0706,  0.5929,  0.3366, -0.2065, -0.0677,  0.4918],\n",
       "                      [ 0.0497, -0.1979, -0.3018,  0.3858, -0.4507, -0.0278, -0.2313,  0.0444,\n",
       "                        0.0488,  0.0213, -0.2284, -0.0271, -0.0095, -0.0210, -0.5705,  0.2200,\n",
       "                       -0.1149, -0.4451,  0.3059,  0.5948,  0.0476, -0.0669, -0.6091, -0.5337,\n",
       "                        0.1213, -0.2893,  0.2953, -0.0822, -0.3382,  0.2522],\n",
       "                      [-0.2934, -0.1492, -0.1619, -0.5975,  0.3294, -0.2883,  0.1977, -0.5288,\n",
       "                        0.1957, -0.1836, -0.1550, -0.5971, -0.3526,  0.0697,  0.0063,  0.2389,\n",
       "                        0.1553,  0.1045, -0.3492, -0.2585,  0.1308,  0.0447,  0.1526, -0.4815,\n",
       "                        0.4635,  0.2482,  0.3472,  0.0170, -0.5096,  0.0091],\n",
       "                      [ 0.1115, -0.1347, -0.3217, -0.0800,  0.1513, -0.0971,  0.2156, -0.4101,\n",
       "                       -0.3371,  0.0873, -0.5105,  0.7427, -0.1101,  0.0886,  0.2102, -0.4397,\n",
       "                        0.1017, -0.1180,  0.3785, -0.2032,  0.0163,  0.2576,  0.1305,  0.1116,\n",
       "                       -0.3829,  0.1167, -0.0714,  0.0920,  0.2807,  0.0031],\n",
       "                      [-0.0176,  0.3522,  0.2457, -0.5613, -0.3827, -0.3876,  0.1053, -0.2653,\n",
       "                        0.3511,  0.1439,  0.0691,  0.3794,  0.2985,  0.0426, -0.0536, -0.0234,\n",
       "                       -0.2015, -0.0671, -0.1084, -0.3221, -0.5400,  0.1109,  0.1399,  0.1270,\n",
       "                        0.3583, -0.3103,  0.1548, -0.6082,  0.2758, -0.0875],\n",
       "                      [-0.2397, -0.4487, -0.0338,  0.0967,  0.3323,  0.0557,  0.1318,  0.0633,\n",
       "                        0.0029, -0.7053,  0.1807, -0.6117, -0.5834, -0.0287,  0.0624,  0.0890,\n",
       "                        0.1418,  0.4981, -0.2388, -0.4887,  0.2367, -0.0346,  0.1289, -0.0190,\n",
       "                        0.1081,  0.2754, -0.1693,  0.2439, -0.4871, -0.3484],\n",
       "                      [ 0.2566, -0.0527, -0.4326,  0.6015, -0.3126, -0.3387,  0.2351, -0.4929,\n",
       "                        0.4024,  0.0912, -0.1568,  0.0261, -0.1185, -0.2361, -0.3979,  0.0203,\n",
       "                        0.0882,  0.0531,  0.0263,  0.2531, -0.6184, -0.2658,  0.1986, -0.2513,\n",
       "                        0.3346, -0.0686,  0.4186, -0.6036,  0.2300, -0.4298]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.2737, -0.6697, -0.2051, -0.4432, -0.0961, -0.4220, -0.1098, -0.1309,\n",
       "                      -0.0017, -0.3777, -0.1936, -0.3113, -0.4254, -0.0965, -0.3839, -0.2051,\n",
       "                      -0.0450, -0.0399, -0.3355, -0.0961], device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.9899, -0.6346, -0.9447, -0.6794, -1.0421,  1.2276, -1.1221, -0.6914,\n",
       "                       -1.5273, -1.1968, -1.0028,  1.5998,  1.3842, -0.4659, -0.7947,  1.3555,\n",
       "                       -0.9873,  1.1137,  1.3576,  1.2260],\n",
       "                      [ 1.0631, -0.3881,  1.4174,  1.2823,  1.2024, -0.8524, -0.0514, -1.5777,\n",
       "                        1.1599, -0.9663,  1.1259,  1.5219,  1.4855, -1.0835,  1.6580, -1.0721,\n",
       "                        0.2392, -0.9423, -0.6921,  1.0926],\n",
       "                      [ 1.3427, -0.5375,  1.0603, -1.0063,  1.2374, -0.8076,  1.3248,  0.9866,\n",
       "                       -0.7986, -0.4635, -1.1305, -0.7356, -0.3826,  1.1014, -0.4161, -0.4325,\n",
       "                        1.3041,  1.0519, -0.4066,  1.1021],\n",
       "                      [-0.6329, -0.4357, -0.5975,  1.5523, -0.8425, -0.8950,  1.4785, -1.1886,\n",
       "                        1.4374,  1.4168, -1.3142, -0.4452, -0.4562, -1.0028, -0.3607, -1.0091,\n",
       "                        1.5461,  1.1727, -1.2460, -0.6451],\n",
       "                      [-0.8643,  1.7735,  1.4719, -0.4551, -0.6959,  1.3568, -1.0753,  1.0236,\n",
       "                       -1.0993, -0.8263,  1.4348, -0.4773, -0.8172,  1.2124,  1.6539, -0.8077,\n",
       "                       -0.8802, -1.1878, -0.8528, -0.7571],\n",
       "                      [-0.5895, -0.3881, -0.5778, -1.0591,  1.2195, -1.0942,  1.0620, -1.0648,\n",
       "                        1.2686,  1.2337, -0.9838, -0.6726, -0.6195,  1.2927, -0.4109,  1.3179,\n",
       "                        1.0842, -1.1812,  1.1805, -0.6379],\n",
       "                      [-0.9278,  1.6137, -0.7829, -1.1233,  1.2309, -0.9262, -0.9817,  0.9497,\n",
       "                        0.9518, -0.7945, -1.3357,  1.4409, -0.9826,  1.1682,  1.3914,  1.2494,\n",
       "                       -0.8207,  1.0825, -0.9463,  1.2103],\n",
       "                      [ 1.3690, -0.4637,  1.3605, -0.6466, -0.4311,  1.2686,  1.3595, -0.7447,\n",
       "                       -1.2767,  0.9568,  1.0284, -0.8905,  1.4311, -0.8440, -0.3611,  0.7267,\n",
       "                        1.3710, -1.0444,  1.1781, -0.6887],\n",
       "                      [ 1.4764, -0.6474, -0.9872,  1.5013, -0.8015, -0.8356, -0.8585,  1.1995,\n",
       "                        1.1679, -1.3582,  1.1524, -0.6557, -0.8584, -0.6375, -0.5989, -0.7364,\n",
       "                       -1.3248,  1.2991, -0.9847, -1.1131],\n",
       "                      [-1.5034, -0.6887, -0.9566,  1.4431, -0.5888,  1.3366, -0.5011,  1.0344,\n",
       "                       -1.3533,  1.2480,  1.2598, -0.3988, -0.6028, -0.9458, -0.7552, -0.8102,\n",
       "                       -1.2512, -1.3089,  1.3882, -0.5026]], device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.1055, -0.2528,  0.6141, -0.0777, -0.1761,  0.4977, -0.2732,  0.1558,\n",
       "                      -0.0925,  0.1395], device='cuda:0'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.112776\n",
      "\n",
      "Accuracy: 9655/10000 (96.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test network\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    if use_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "    output = model(data)\n",
    "    \n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    test_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    _, pred = torch.max(output, 1)\n",
    "    \n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "# Average\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "print(\"Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "    correct, len(test_loader.dataset), \n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
